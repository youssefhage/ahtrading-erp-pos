services:
  # NOTE: Dokploy attaches routed services (api/admin) to a shared network
  # (`dokploy-network`). To keep DNS resolution working (api -> db) we run all
  # services on that same network, and we use a unique DB service name to avoid
  # collisions with other stacks.
  db_ahtrading_pos:
    image: postgres:15
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      # Pilot reset: use a fresh volume so we start from an empty DB.
      - pgdata_pilot_20260210:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U \"$POSTGRES_USER\" -d \"$POSTGRES_DB\""]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 20s
    networks:
      - default
      - dokploy

  # Name this service uniquely to avoid any Traefik/Dokploy routing collisions with other stacks.
  api_melqard:
    build:
      # This file lives in `deploy/`, so set context to repo root.
      context: ..
      dockerfile: backend/Dockerfile
    restart: unless-stopped
    environment:
      APP_ENV: ${APP_ENV:-prod}
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db_ahtrading_pos:5432/${POSTGRES_DB}
      # Non-superuser app role used by API/worker so RLS is actually enforced.
      APP_DB_USER: ${APP_DB_USER}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}
      APP_DATABASE_URL: postgresql://${APP_DB_USER}:${APP_DB_PASSWORD}@db_ahtrading_pos:5432/${POSTGRES_DB}
      # Edge -> cloud replication auth (cloud-side; tenant-scoped).
      # Single-tenant: EDGE_SYNC_KEY + EDGE_SYNC_COMPANY_ID
      # Multi-tenant: EDGE_SYNC_KEY_BY_COMPANY (JSON/CSV map of company_id -> key)
      EDGE_SYNC_KEY: ${EDGE_SYNC_KEY:-}
      EDGE_SYNC_COMPANY_ID: ${EDGE_SYNC_COMPANY_ID:-}
      EDGE_SYNC_KEY_BY_COMPANY: ${EDGE_SYNC_KEY_BY_COMPANY:-}
      EDGE_SYNC_NODE_COMPANY_MAP: ${EDGE_SYNC_NODE_COMPANY_MAP:-}
      # Browser/mobile clients (future): allow these origins.
      CORS_ORIGINS: ${CORS_ORIGINS:-https://app.melqard.com,https://pos.melqard.com}
      # Safety: do not auto-create/reset admin users in production.
      BOOTSTRAP_ADMIN: ${BOOTSTRAP_ADMIN:-0}
      BOOTSTRAP_ADMIN_EMAIL: ${BOOTSTRAP_ADMIN_EMAIL:-}
      BOOTSTRAP_ADMIN_PASSWORD: ${BOOTSTRAP_ADMIN_PASSWORD:-}
      BOOTSTRAP_ADMIN_RESET_PASSWORD: ${BOOTSTRAP_ADMIN_RESET_PASSWORD:-0}
      # CI publishing of Tauri update artifacts (written into the shared downloads volume).
      UPDATES_DIR: /updates
      UPDATES_PUBLISH_KEY: ${UPDATES_PUBLISH_KEY:-}
    depends_on:
      db_ahtrading_pos:
        condition: service_healthy
    healthcheck:
      # Dokploy uses container health to gate deployments. Use a liveness-style
      # endpoint so we can still bring the stack up and then diagnose DB issues
      # separately via /health (readiness).
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/meta').read()"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 180s
    command:
      - sh
      - -lc
      - |
        set -euo pipefail
        # Temporary cleanup: hard-delete unofficial leftovers from official company.
        # This block is intentionally one-time and will be removed after verification.
        python3 - <<'PY'
        import os
        import psycopg
        from psycopg import sql

        targets = [
          "UN-134", "UN-135", "UNBAR-009", "UNBAR-010", "UNBAR-011",
          "UNBAR-012", "UNBAR-013", "UNBAR-014", "UNCRO-002", "UNCRO-003",
          "UNDIP-001", "UNFAR-001", "UNNES-005", "UNSPA-003", "UNTMO-001",
        ]

        db_url = os.environ.get("APP_DATABASE_URL") or os.environ.get("DATABASE_URL")
        if not db_url:
          raise SystemExit("cleanup: missing DB URL")

        try:
          with psycopg.connect(db_url) as conn:
            with conn.cursor() as cur:
              cur.execute("SELECT id FROM companies WHERE name=%s LIMIT 1", ("AH Trading Official",))
              row = cur.fetchone()
              if not row:
                raise RuntimeError("cleanup: AH Trading Official not found")
              official_id = row[0]

              cur.execute(
                "CREATE TEMP TABLE target_items ON COMMIT DROP AS "
                "SELECT id, sku FROM items WHERE company_id=%s AND sku = ANY(%s)",
                (official_id, targets),
              )
              cur.execute("SELECT COUNT(*) FROM target_items")
              print(f"cleanup target_items={cur.fetchone()[0]}", flush=True)

              cur.execute(
                """
                SELECT kcu.table_schema, kcu.table_name, kcu.column_name
                FROM information_schema.table_constraints tc
                JOIN information_schema.key_column_usage kcu
                  ON tc.constraint_name = kcu.constraint_name
                 AND tc.table_schema = kcu.table_schema
                JOIN information_schema.constraint_column_usage ccu
                  ON ccu.constraint_name = tc.constraint_name
                 AND ccu.table_schema = tc.table_schema
                WHERE tc.constraint_type = 'FOREIGN KEY'
                  AND ccu.table_schema = 'public'
                  AND ccu.table_name = 'items'
                  AND ccu.column_name = 'id'
                  AND kcu.table_schema = 'public'
                  AND kcu.table_name <> 'items'
                """
              )
              refs = cur.fetchall()
              for schema_name, table_name, col_name in refs:
                q = sql.SQL(
                  "DELETE FROM {}.{} child USING target_items t WHERE child.{} = t.id"
                ).format(
                  sql.Identifier(schema_name),
                  sql.Identifier(table_name),
                  sql.Identifier(col_name),
                )
                cur.execute(q)

              cur.execute("DELETE FROM items i USING target_items t WHERE i.id = t.id")
              print(f"cleanup deleted_items={cur.rowcount}", flush=True)
            conn.commit()
        except Exception as exc:
          # Never block API startup if cleanup fails; we'll verify and handle separately.
          print(f"cleanup error={exc}", flush=True)
        PY
        # Run migrations/seeds in the background so the API can boot and become routable
        # (Dokploy/compose deployments may fail if we block on init_db).
        (
          set -euo pipefail
          attempt=0
          until backend/scripts/init_db.sh; do
            attempt=$$((attempt+1))
            if [ "$$attempt" -ge 60 ]; then
              echo "init_db failed after $$attempt attempts" >&2
              exit 1
            fi
            echo "init_db failed; retrying in 2s ($$attempt/60)..." >&2
            sleep 2
          done
        ) &
        exec uvicorn backend.app.main:app --host 0.0.0.0 --port 8000
    volumes:
      - downloads_updates:/updates
    # Intentionally do not set networks here. Dokploy rewrites routed services
    # (like `api` / `admin`) to its own routing network. In practice we've seen
    # cases where the rewrite results in the API not being able to reach Postgres
    # during /health, which blocks the entire deployment. To make networking
    # deterministic, explicitly attach routed services to both networks.
    networks:
      - default
      - dokploy

  worker:
    build:
      context: ..
      dockerfile: backend/Dockerfile
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db_ahtrading_pos:5432/${POSTGRES_DB}
      APP_DB_USER: ${APP_DB_USER}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}
      APP_DATABASE_URL: postgresql://${APP_DB_USER}:${APP_DB_PASSWORD}@db_ahtrading_pos:5432/${POSTGRES_DB}
    # Don't hard-block startup on API health. If the API is still initializing
    # (migrations/seeds), we still want the stack to come up so we can debug via
    # logs and UI instead of Dokploy/compose failing the whole deployment.
    depends_on:
      db_ahtrading_pos:
        condition: service_healthy
    command:
      - sh
      - -lc
      - |
        set -euo pipefail
        attempt=0
        until backend/scripts/init_db.sh; do
          attempt=$$((attempt+1))
          if [ "$$attempt" -ge 60 ]; then
            echo "init_db failed after $$attempt attempts" >&2
            exit 1
          fi
          echo "init_db failed; retrying in 2s ($$attempt/60)..." >&2
          sleep 2
        done
        exec python3 -m backend.workers.worker_service --db "$$APP_DATABASE_URL"
    networks:
      - default
      - dokploy

  admin:
    build:
      context: ..
      dockerfile: apps/admin/Dockerfile
      args:
        # Cloud pilot: use a same-origin proxy prefix to avoid path/host collisions.
        NEXT_PUBLIC_API_BASE_URL: /api
        API_PROXY_TARGET: http://api_melqard:8000
    restart: unless-stopped
    environment:
      API_PROXY_TARGET: http://api_melqard:8000
    # Admin can start even if API isn't healthy yet (it will just show API errors).
    # This avoids app.melqard.com going dark during API init/debug.
    depends_on:
      - api_melqard
    networks:
      - default
      - dokploy

  # Static downloads/update host for desktop apps (Tauri installers + updater manifests).
  downloads:
    build:
      context: ..
      dockerfile: apps/downloads/Dockerfile
    restart: unless-stopped
    environment:
      # No functional impact; used to force Dokploy to re-apply/rebuild the downloads
      # container when we change how the landing page is served.
      DOWNLOADS_SITE_FROM_UPDATES: "1"
    volumes:
      - downloads_updates:/updates
    networks:
      - default
      - dokploy

  # Web POS (browser fallback / quick pilot access).
  pos_web:
    build:
      context: ..
      dockerfile: apps/pos-web/Dockerfile
    restart: unless-stopped
    networks:
      - default
      - dokploy

volumes:
  pgdata_pilot_20260210:
  downloads_updates:

networks:
  # Default per-compose network (still used by worker/db for isolation).
  default:
    driver: bridge
  # Shared Dokploy routing network used by Traefik (and routed services).
  dokploy:
    external: true
    name: dokploy-network
